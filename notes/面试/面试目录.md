## 一、知识汇总

- [01｜Java核心底层技术](01｜Java核心底层技术.md)
- [02｜Spring生态体系与微服务](02｜Spring生态体系与微服务.md)
- [03｜数据库、缓存与中间件](03｜数据库、缓存与中间件.md)
- [04｜容器化与云原生](04｜容器化与云原生.md)
- [05｜综合实战](05｜综合实战.md)

## 二、知识补充

#### Q1：JVM 性能调优实战：当线上服务出现频繁 Full GC，你的排查思路是什么？

- **考察点**：监控工具使用（jstat, jmap）、内存区域分布、GC 日志分析。
    
- **深度角度**：如何通过 `dump` 文件定位是内存泄漏还是大对象分配过快？在 K8s 容器环境下，JVM 参数如何适配容器的资源限制（Cgroups）以防止 OOM Kill？
    
- **标准回答**：
    
    - **监控定位**：通过 Prometheus/Zabbix 观察堆内存曲线。若回收后内存位置持续抬升，怀疑**内存泄漏**；若曲线剧烈波动，怀疑**短命大对象**过多。
        
    - **工具排查**：使用 `jstat -gcutil` 观察各区耗时。执行 `jmap -dump` 导出堆快照。
        
    - **分析原因**：利用 MAT 或 VisualVM 分析 `dump` 文件，查看 `Retained Heap` 最大的对象。
        
    - **调整优化**：若是代码问题（如静态集合未清理）则修复代码；若是参数问题，调整 `-Xmx` 或 `-XX:MaxGCPauseMillis`。
        
- **架构深度**：在 K8s 中，必须开启 `-XX:+UseContainerSupport` 确保 JVM 识别容器配额，否则 JVM 可能按宿主机内存分配，导致被容器 `OOM Kill`。
    
#### Q2：多线程协同：如何利用 AQS 实现一个自定义的同步组件？

- **考察点**：`AbstractQueuedSynchronizer` 的原理、CAS、线程排队。
    
- **深度角度**：对比 `ReentrantLock` 和 `synchronized` 在高并发下的锁升级过程及性能差异；如何处理线程池满时的拒绝策略以保证业务不丢失？
    
- **标准回答**： AQS 是 JUC 的核心，通过一个 `volatile int state` 状态位和 `CLH` 双向变体队列实现。获取锁失败的线程会被封装成 Node 加入队列并阻塞（LockSupport.park）。
    
- **架构深度**：
    
    - **synchronized**：从偏向锁 -> 轻量级锁 -> 重量级锁。适合竞争不激烈的场景，由 JVM 自动管理。
        
    - **ReentrantLock**：基于 AQS，支持公平/非公平锁、可中断、超时。高并发下，由于其具备更多的灵活性且避免了过多的上下文切换，通常表现更稳健。
        

#### Q3：Spring Cloud 组件选型：Eureka 停更后，为什么 Nacos 逐渐成为主流？

- **考察点**：服务注册与发现原理、配置中心。
    
- **深度角度**：对比 Nacos 的 CP 和 AP 模式切换逻辑。在微服务架构中，如果注册中心挂了，服务间还能相互调用吗？（考察客户端缓存机制）。
    
- **标准回答**： Nacos 优势在于**配置+发现一体化**，且支持 CP（Raft）和 AP（Distro）模式切换。
    
- **架构深度**： 若 Nacos 宕机，由于客户端存在**本地注册表缓存**，只要不重启，服务间依然可以根据本地缓存的 IP 列表进行调用。配合 Ribbon/LoadBalancer 的重试机制，可保证核心链路不中断。
    

#### Q4：分布式事务解决：在你的项目中，是如何处理跨服务的数据一致性？

- **考察点**：Seata (AT/TCC/Saga)、RocketMQ 事务消息。
    
- **深度角度**：为什么说在互联网高并发场景下，追求“最终一致性”优于“强一致性”？TCC 模式下的空回滚和幂等问题如何解决？
    
- **标准回答**： 高并发场景下首选 **RocketMQ 事务消息**。
    
    - 发送 Half Message。
        
    - 执行本地事务。
        
    - Commit/Rollback。
        
- **架构深度**： TCC 模式存在“空回滚”风险。**解决方案**：在事务记录表中增加状态位，执行 `Cancel` 前先检查 `Try` 是否已成功执行。原则上，核心链路用事务消息，非核心链路用消息补偿。
    

#### Q5：MySQL 索引与执行计划：如何利用 `EXPLAIN` 发现隐藏的性能瓶颈？

- **考察点**：索引失效场景、回表、索引下推、最左匹配。
    
- **深度角度**：对于分库分表后的 SQL，如果涉及跨库排序（Order By）和聚合（Group By），执行计划会有什么不同？中间件是如何重写 SQL 的？
    
- **标准回答**： 关注 `type`（目标 `ref/range`，严禁 `ALL`）、`rows`（扫描行数）、`extra`（关注 `Using filesort/Temporary`）。
    
- **架构深度**： 分库分表后，若 SQL 未带分片键，会导致**全路由扫描**。此时 `type` 虽可能为 `index`，但扫描总量（各个分表之和）巨大，极易拖垮主库。
    

#### Q6：分库分表实战：ShardingSphere 的分片策略你如何选择？

- **考察点**：分片键选型、垂直拆分与水平拆分。
    
- **深度角度**：当数据量进一步膨胀需要二次扩容时，如何实现数据平滑迁移？如何处理全局唯一 ID 生成（雪崩算法、UUID、号段模式）？
    
- **标准回答**： 选型通常基于 `user_id` 或 `order_id`。
    
- **架构深度**： **扩容方案**：1. 修改分片规则（如从 4 库变 8 库）；2. 开启 Binlog 双写同步；3. 数据校验一致后切换。**ID 生成**：首选雪崩算法（Snowflake），需注意 K8s 动态扩容时 `WorkerId` 的分配，可通过 Redis 或 ZK 动态注册。
    

#### Q7：Redis 持久化与一致性：AOF 和 RDB 在生产环境如何平衡？

- **考察点**：持久化机制、数据安全性。
    
- **深度角度**：在“先写数据库再删缓存”的策略下，如果删除缓存失败了怎么办？如何通过 Canal 监听 Binlog 配合 MQ 实现异步删除补偿？
    
- **标准回答**： 主流方案是 **Cache** **Aside Pattern**：先更新 DB，再删除缓存。
    
- **架构深度**： 为防止删除失败，可利用 **Canal 监听 Binlog** 发送到 MQ，由消费者重试删除。这是目前大厂保证最终一致性的“金标准”。
    

#### Q8：Redis 分布式锁：Redlock 真的安全吗？在高并发下如何防止锁过期？

- **考察点**：`SETNX`、过期时间。
    
- **深度角度**：详细解析 Watchdog（看门狗）机制的实现；在 Cluster 集群模式下，如果主节点宕机但锁未同步到从节点，导致锁失效怎么处理？
    
- **标准回答**： Redisson 的 `Watchdog` 每 10s 检查一次，若业务未完则延长锁时间。
    
- **架构深度**： Redlock 解决了单点故障，但在极端网络分区或时钟偏移下仍有风险。**高并发优化**：将锁粒度细化（如由锁整表改为锁具体行 ID），利用 Lua 脚本保证多步操作的原子性。
    

#### Q9：消息可靠性：RocketMQ 是如何保证消息百分之百不丢失的？

- **考察点**：生产者同步发送、Broker 同步刷盘、消费者 ACK。
    
- **深度角度**： 对比 Kafka 和 RocketMQ 在处理海量 Topic 时的性能表现差异及其存储模型原因。
    
- **标准回答**：
    
    - **发送**：同步发送 + 重试。
        
    - **Broker**：同步刷盘（SYNC_FLUSH）+ 多副本同步复制。
        
    - **消费**：手动提交 Offset，业务执行完再确认。
        
- **架构深度**： RocketMQ 的 `CommitLog` 顺序写模型使其在海量 Topic 下比 Kafka 更稳健（Kafka 的分区过多会导致随机读写）。
    

#### Q10：消息幂等与积压：如果 MQ 消费端出现了千万级积压，你如何紧急处理？

- **考察点**：消费者扩容、分区逻辑。
    
- **深度角度**：在业务逻辑层面，如何设计一套通用的幂等 SDK（利用数据库唯一键或 Redis）来应对重复投递？
    
- **标准回答**：
    
    - **临时扩容**：将老 Topic 的消息转发到拥有更多 Queue 的新 Topic。
        
    - **增加 Consumer**：部署 10 倍的机器进行消费。
        
- **架构深度**： **幂等 SDK 设计**：在消息体中携带全局 `msgId`，消费前在 Redis 执行 `SETNX msgId 1 EX 86400`。若存在则直接跳过。
    

#### Q11：K8s 部署实战：容器的健康检查（Liveness 与 Readiness）有什么区别？

- **考察点**：Pod 生命周期管理、服务平滑滚动更新。
    
- **深度角度**：在 Openshift 环境下，如何配置 HPA（水平自动扩缩容）基于 CPU/内存指标？当应用发布新版本时，如何实现“零停机”下线旧连接？
    
- **标准回答**：
    
    - **Liveness**：决定容器是否重启。
        
    - **Readiness**：决定是否接入流量。
        
- **架构深度**： 平滑下线需配合 `preStop` 钩子，先从 Service 摘除 Pod，再执行 `sleep 20s` 让存量请求处理完。
    

#### Q12：灰度发布与流量治理：Spring Cloud Gateway 如何配合 K8s 实现蓝绿发布？

- **考察点**：网关动态路由、流量染色。
    
- **深度角度**：如何通过网关层的谓词工厂（Predicate）和过滤器（Filter）根据 UserID 实现 1% 的灰度流量导入？
    
- **标准回答**： 通过 Gateway 拦截请求，解析 Header 中的 `version` 或 `user_id`，利用路由权重（Weight）分配。
    
- **架构深度**： **全链路灰度**：需要将灰度标识透传到下游 RPC 和 MQ，让整条链路都流向灰度节点。
    

#### Q13：秒杀系统设计：如何利用 Redis + MQ 抗住瞬时百万级流量？

- **考察点**：削峰填谷、预减库存。
    
- **深度角度**： 如何防止秒杀接口被刷？如何保证 Redis 预减库存与数据库实际库存最终一致？
    
- **标准回答**：
    
    - **前置限流**。
        
    - **Redis 预减库存**（Lua 脚本）。
        
    - **MQ 异步下单**。
        
- **架构深度**： **防止超卖**：数据库层执行 `update set stock=stock-1 where id=x and stock>0` 兜底。
    

#### Q14：服务容错与保护：Sentinel 的限流和熔断算法原理是什么？

- **考察点**：滑动窗口、令牌桶、漏桶算法。
    
- **深度角度**：当某个服务被熔断后，如何实现自动恢复测试流量？在高并发环境下，限流参数如何通过全链路压测动态得出？
    
- **标准回答**： 基于**滑动窗口算法**。熔断支持 RT、异常比例、异常数。
    
- **架构深度**： 熔断后进入“半开状态”，允许少量请求探测，若成功则恢复。限流阈值应通过全链路压测的 QPS 峰值来动态调整。
    

#### Q15：架构师思维：如何设计一个高性能、可扩展的分布式配置中心？

- **考察点**：配置下发、长轮询机制。
    
- **深度角度**：如果让你重构 Apollo 或 Nacos 的核心逻辑，如何保证成千上万个客户端在配置变更时的实时通知且不拖垮服务端？
    
- **标准回答**： 基于 **长轮询（Long Polling）** 机制。客户端请求服务器，若配置无变化，服务端挂起请求（如 29s），有变化则立即返回。
    
- **架构深度**： 这种方式既保证了实时性，又避免了短轮询对 CPU 的浪费。

#### Q001：如何解决分库分表后的全局分布式 ID 冲突及非分片键查询问题？

- **考察点**：分布式 ID 生成策略、异构索引表、数据冗余。
    
- **深度角度**：如何在保证 ID 绝对单调递增（有利于 B+ Tree 写入性能）的同时，解决多维度的查询诉求。
    
- **标准回答**：
    
    - 使用 **Snowflake（雪花算法）** 或 **号段模式** 生成全局唯一 ID。
        
    - 针对非分片键（如手机号查订单，而分片键是用户 ID），采用 **映射表（Mapping Table）**、**基因法（Gene）** 或 **ES 异构索引**。
        
- **详细解析**： 雪花算法产生的 ID 包含时间戳，能保证索引页插入的顺序性，减少页分裂。基因法是指将 UID 的后几位提取出来作为索引基因，拼接到订单 ID 中，使得同一个用户的订单 ID 和用户 ID 经过 Hash 后落入同一个分片。
    
- **架构深度**： 在 K8s 环境下部署雪花算法需注意 `workId` 冲突，可利用 Redis 的 `INCR` 指令或 Kubernetes 的 `StatefulSet` 序号作为 `workId` 来源。对于超大规模查询，通常采用 **Canal + MQ + ES** 的架构，将写库后的增量数据同步到 ES 中进行多维搜索。
    

#### Q002：JVM 在容器（K8s）中运行，为什么经常出现内存溢出或被 OOM Killer 杀死？

- **考察点**：容器资源限制（Cgroups）、JVM 堆参数适配、Metaspace 调优。
    
- **深度角度**：JVM 感知宿主机 CPU/内存与容器 Quota 差异导致的线程数和堆分配错误。
    
- **标准回答**：
    
    - 早期 JDK 不感知 Cgroups 限制，默认按宿主机内存分配堆，导致 Pod 超过 Limit 被内核杀死。
        
    - 应使用 JDK 8u191+ 及其自带的 `-XX:+UseContainerSupport` 参数。
        
- **详细解析**：
    
    若 Pod Limit 设为 2G，不加适配的 JVM 可能按宿主机 64G 的 1/4（16G）分配堆。解决办法是显式设置 `-XX:MaxRAMPercentage=75.0`。
    
- **架构深度**： 除了堆，还需关注 **Native Memory**。例如使用 Netty 产生的堆外内存或 K8s 的 Sidecar（如 Istio）占用的开销。建议在监控中通过 `NMT (Native Memory Tracking)` 观察内存各区分布。
    

#### Q003：Spring Cloud 架构下，如何处理流量突增导致的“雪崩”及服务自愈？

- **考察点**：熔断限流策略、半开状态、线程隔离。
    
- **深度角度**：对比 Sentinel 的滑动窗口算法与 Hystrix 的线程隔离，以及自愈能力的实现。
    
- **标准回答**：
    
    - 使用 Sentinel 进行热点限流、冷启动预热（Warm Up）和熔断降级。
        
    - 配合 K8s 的 HPA 进行动态扩容，实现基础设施层的自愈。
        
- **详细解析**： 熔断机制通过监控异常比例或响应时间，当达到阈值时切断调用，返回兜底数据。自愈则是在熔断一段时间后进入 **Half-Open** 状态，允许少量流量探测，若成功则关闭熔断。
    
- **架构深度**：
    
    在分布式环境下，单纯的本地熔断是不够的。应结合 **全链路压测** 得出的阈值，在网关层通过 Redis 实现分布式限流，防止瞬间流量冲垮下游服务。
    

#### Q004：Redis 分布式锁在主从切换时失效（锁丢失）如何彻底解决？

- **考察点**：Redis 主从同步异步性、Redlock 算法、一致性代价。
    
- **深度角度**：分布式锁的可靠性与系统吞吐量的权衡。
    
- **标准回答**：
    
    - 主从架构下，Master 未同步到 Slave 就宕机会导致锁丢失。
        
    - 方案：1. 使用 **Redlock** 算法（多点写入）；2. 采用 **Zookeeper** 实现强一致性锁。
        
- **详细解析**： Redlock 要求客户端向 5 个独立的 Redis 实例请求锁，过半成功才算获取。虽然解决了丢失问题，但增加了网络开销和系统复杂性。
    
- **架构深度**： 在实际生产中，若业务允许极低概率的重复（如幂等操作），通常维持主从加 **Watchdog** 即可。若涉及资金结算，必须使用 Zookeeper 这种基于 ZAB 协议的强一致性方案。
    

#### Q005：消息中间件（RocketMQ）如何保证在分布式事务中的最终一致性？

- **考察点**：事务消息机制、Half Message、回查逻辑。
    
- **深度角度**：消息发送原子性与业务事务原子性的解耦方案。
    
- **标准回答**：
    
    - 通过 **发送半消息 -> 执行本地事务 -> 二次确认（Commit/Rollback）** 的流程。
        
    - Broker 会对超时的半消息进行 **状态回查**。
        
- **详细解析**： 半消息对消费者不可见。只有生产者确认提交后，消费者才能拉取。回查逻辑解决了生产者宕机导致消息状态不明的问题。
    
- **架构深度**：
    
    这种模式下，下游消费必须保证 **幂等**。架构上需考虑消息堆积时的“死信队列”处理，确保链路最终闭环。
    

#### Q006：Spring Cloud 动态配置中心（Nacos/Apollo）的配置下发原理及性能开销？

- **考察点**：长轮询（Long Polling）、配置版本管理、监听机制。
    
- **深度角度**：在万级客户端连接下，如何保证配置变更的实时性而不拖垮 Server。
    
- **标准回答**：
    
    - 基于 **长轮询机制**：客户端发起 30s 请求，Server 侧若配置变化则立即返回，否则 Hold 住连接直至超时。
        
- **详细解析**： 相比推送（Push）的复杂性（连接易断）和短轮询（Pull）的高压，长轮询兼顾了实时性和服务端压力。
    
- **架构深度**： 需配置合理的 JVM 线程池处理 Server 侧挂起的请求。在 K8s 中，配置中心常与 ConfigMap 结合，或通过 Sidecar 动态注入配置，实现业务无感知更新。
    

#### Q007：分库分表后，如何实现跨库的 Join 查询和分页聚合？

- **考察点**：分布式查询优化、ER 表分片一致性、内存计算。
    
- **深度角度**：SQL 执行引擎的逻辑重写与性能损耗。
    
- **标准回答**：
    
    - 1. **绑定表（Binding Table）**：将父子表按相同分片键分布在同一节点，转为单节点 Join。
            
    - 2. **数据冗余**：将常用字段冗余到各表。
            
    - 3. **异构索引**：同步到 ES 或 ClickHouse 进行多维分析。
            
- **详细解析**： 跨库分页（如 `LIMIT 1000000, 10`）是性能杀手，中间件通常需要拉取所有库的前 100万+10条数据在内存合并，极易 OOM。
    
- **架构深度**： 解决跨库分页的终极方案是 **“禁止跨库深分页”**，改用“滚动查询（Search After）”，即记录上次查询的最大 ID，通过 `WHERE id > last_id` 进行过滤。
    

#### Q008：Redis 大 Key、热 Key 问题在线上环境如何快速定位与解决？

- **考察点**：内存分布监控、Redis 命令执行原理、热点探测。
    
- **深度角度**：在不阻塞主线程的前提下进行热点 Key 的动态迁移。
    
- **标准回答**：
    
    - **大 Key**：使用 `SCAN` 命令扫描或分析 RDB 文件；采用 `UNLINK` 异步删除。
        
    - **热 Key**：利用 Redis 4.0+ 的 `LFU` 策略或热点探测组件（如京东 HotKey）；使用本地缓存（Caffeine）挡一层。
        
- **详细解析**：
    
    大 Key 最大的危害是序列化耗时和阻塞 IO，热 Key 则是压垮单节点 CPU。解决热 Key 可以将 Key 加上随机后缀分散到不同 Slot。
    
- **架构深度**： 在 K8s 中，可利用 Service Mesh 拦截 Redis 流量，在 Sidecar 层统计 Key 的访问频率，实现自动化的热点探测与动态本地缓存加载。
    

#### Q009：MySQL 主从延迟严重，如何保证关键业务（如刚下单就要查到）的读写一致性？

- **考察点**：主从复制原理（Binlog）、读写分离缺陷、同步复制策略。
    
- **深度角度**：强一致性读取的多种权衡方案。
    
- **标准回答**：
    
    - 1. **强制读主**：关键业务代码路由到 Master。
            
    - 2. **判断有效性**：比较 Binlog 位点，确认 Slave 已同步。
            
    - 3. **缓存同步**：写库后同时写一份数据到 Redis。
            
- **详细解析**：
    
    默认异步复制会导致几百毫秒延迟。若使用全同步复制（Group Replication），则写入性能会因网络确认而骤降。
    
- **架构深度**： 架构师通常采用 **“延迟读取策略”**。对于非强一致要求的 UI 展示，允许几秒延迟；对于核心逻辑（如支付回调），采用强制读主。
    

#### Q010：K8s 环境下，Pod 滚动更新（Rolling Update）如何做到真正的零停机？

- **考察点**：Pod 生命周期、优雅停机（PreStop）、Readiness Probe。
    
- **深度角度**：负载均衡摘除延迟与 JVM 停机耗时的同步。
    
- **标准回答**：
    
    - 配置 `ReadinessProbe` 确保 Pod 就绪再接入流量。
        
    - 配置 `PreStop` 钩子，让应用先 `sleep` 几十秒再关闭，确保负载均衡（Ingress/Service）有时间摘除节点。
        
- **详细解析**： 若直接 `SIGTERM` 关闭 JVM，由于 K8s 控制平面通知 Ingress 摘除节点有延迟，新请求仍会发送到正在关闭的 Pod，导致 502。
    
- **架构深度**： 在 Spring Cloud 应用中，还需考虑注册中心（如 Nacos）的下线延迟。应在 `PreStop` 中显式调用微服务下线接口，将本地实例状态设为 `DOWN`，加速调用方感应。
    

#### Q011：为什么 Kafka 在海量 Topic 下性能会急剧下降，而 RocketMQ 能扛得住？

- **考察点**：存储模型差异、顺序 IO、Random IO。
    
- **深度角度**：磁盘索引文件结构对 IOPS 的消耗。
    
- **标准回答**：
    
    - **Kafka**：每个 Partition 对应一个物理文件。Topic 越多，文件越多，顺序写退化为随机写。
        
    - **RocketMQ**：所有消息写一个 `CommitLog` 文件。顺序写优势更持久。
        
- **详细解析**： Kafka 追求极致的单 Topic 吞吐量。RocketMQ 通过 `ConsumeQueue` 索引文件与 `CommitLog` 数据文件分离，实现了单机万级 Topic 依然保持顺序 IO。
    
- **架构深度**：
    
    在金融级架构中，RocketMQ 的同步刷盘和多副本确认更能保证不丢数据，虽然吞吐略逊于 Kafka。
    

#### Q012：如果线上 Redis 内存突然飙升，你的排查思路和应急方案是什么？

- **考察点**：内存淘汰策略、过期删除机制、Info memory 命令。
    
- **深度角度**：如何区分是数据增长、碎片率过高还是缓冲区溢出。
    
- **标准回答**：
    
    - 1. 执行 `info memory` 查看 `used_memory_rss` 与 `used_memory` 的比值。
            
    - 2. 检查 `client list` 确认是否有由于消费慢导致的输出缓冲区（Output Buffer）堆积。
            
    - 3. 检查大 Key 和过期策略。
            
- **详细解析**： 碎片率过高可通过手动 `memory purge` 或开启 `activedefrag` 解决。应急方案：调大 `maxmemory` 或修改淘汰策略为 `allkeys-lru`。
    
- **架构深度**： 在 K8s 中需警惕 **Swap 交换**。若 Redis 使用了 Swap，响应时间会从微秒降到毫秒级。应通过 `Cgroups` 禁用 Redis Pod 的 Swap 使用。
    

#### Q013：Spring Cloud Gateway 的高性能原理是什么？在高并发下如何调优？

- **考察点**：Netty、Reactor 模型、非阻塞 IO。
    
- **深度角度**：响应式编程模型（WebFlux）对线程模型的改变。
    
- **标准回答**：
    
    - 底层采用 **Netty + Project Reactor**。
        
    - 调优手段：调整 Netty 线程数、优化过滤器执行链（避免在 Filter 中做阻塞操作）。
        
- **详细解析**：
    
    传统 Zuul 1.x 采用线程池隔离，每个请求占一个线程。Gateway 少量线程即可处理成千上万连接。
    
- **架构深度**： 网关限流应配合 Redis Lua 脚本。若业务逻辑复杂，严禁在网关内写 DB，应通过异步 RPC 调用。
    

#### Q014：如何设计一个能支撑 10 万并发的点赞计数系统，并保证数据不丢失？

- **考察点**：写缓冲（Write Behind）、原子计数、数据持久化落库。
    
- **深度角度**：读写吞吐量与最终一致性的平衡。
    
- **标准回答**：
    
    - **写**：流量进入 Redis，使用 `INCR` 原子递增。
        
    - **读**：直接读 Redis。
        
    - **落库**：通过 MQ 异步分批刷回数据库（MySQL）。
        
- **详细解析**： 为防止消息丢失，可采用“双写 + 校验”。Redis 宕机时，利用 MQ 消息重放恢复数据。
    
- **架构深度**： 极端高并发下，可以使用 **HyperLogLog** 进行基数统计（若不需要精确 ID 点赞列表），内存占用极低。
    

#### Q015：K8s 环境下，如何实现微服务之间的全链路追踪与性能监控？

- **考察点**：分布式链路追踪（Skywalking/Zipkin）、侧车模式（Sidecar）、数据聚合。
    
- **深度角度**：TraceID 在多线程、异步调用及跨协议间的透传。
    
- **标准回答**：
    
    - 使用 **Skywalking** 字节码增强技术，无侵入采集 Trace 指标。
        
    - 利用 Prometheus + Grafana 采集容器及业务 Metrics。
        
- **详细解析**：
    
    TraceID 必须在 HTTP Header 和 RPC Context 中传递。对于线程池异步任务，需使用修饰后的线程池进行变量透传。
    
- **架构深度**： 在 Openshift/K8s 环境，可结合 **Service Mesh (Istio)**。Istio 原生支持流量监控，通过 Envoy 自动生成链路信息，业务代码完全零入侵。

#### Q016：HashMap 在多线程环境下除了死循环（JDK 7），还会产生什么问题？如何深度优化？

- **考察点**：哈希碰撞、红黑树转换、线程安全替代方案。
    
- **深度角度**：分析 ConcurrentHashMap 在 JDK 8 中锁粒度的进化（从 Segment 到 Synchronized+CAS）。
    
- **标准回答**：
    
    - **问题**：除了 JDK 7 的死循环，多线程下还会出现**数据覆盖（Data Overwrite）**、**Size 计算不准**等问题。
        
    - **优化**：使用 `ConcurrentHashMap`。它放弃了分段锁，改为使用 `CAS` 操作和 `synchronized` 锁住桶节点（Node）。
        
- **详细解析**：
    
    JDK 8 版本的 `ConcurrentHashMap` 锁粒度更细。在插入数据时，如果桶为空，直接用 `CAS` 插入；如果不为空，只锁住当前的 `Node` 头节点，不影响其他桶的读写。
    
- **架构深度**： 在超高并发只读场景下，可以考虑 `CopyOnWriteArrayList` 的思想，但要注意其写操作的开销。对于大数据的计数，`LongAdder` 的性能由于采用了分段累加，通常优于 `AtomicLong`。
    

#### Q017：在高并发网关或文件传输系统中，为什么 Netty 的零拷贝（Zero-copy）比传统 IO 快得多？

- **考察点**：直接内存（Direct Memory）、内核缓冲区映射、mmap。
    
- **深度角度**：数据在用户态与内核态之间切换的上下文开销。
    
- **标准回答**：
    
    - 传统 IO 需要 4 次拷贝和 4 次上下文切换。
        
    - Netty 零拷贝通过 **Direct Buffer** 直接操作物理内存，或通过 `FileChannel.transferTo` 调用系统的 `sendfile`。
        
- **详细解析**：
    
    传统 IO：磁盘 -> 内核缓冲区 -> 用户缓冲区 -> Socket 缓冲区 -> 网卡。 零拷贝：磁盘 -> 内核缓冲区 -> 网卡（通过 `sendfile` 指令），数据完全不经过 JVM 堆。
    
- **架构深度**： 使用直接内存虽快，但其分配和释放开销大且不受 JVM 垃圾回收直接控制。Netty 引入了**内存池（PooledByteBufAllocator）**技术来复用这些内存，防止内存泄漏，这在处理高吞吐量的 K8s 网关流量时至关重要。
    

#### Q018：Java 线程池中，当任务量瞬间超过 `maxPoolSize` 时，饱和策略（RejectedExecutionHandler）如何设计能保证业务不丢失？

- **考察点**：线程池工作流、阻塞队列、拒绝策略定制。
    
- **深度角度**：如何结合消息中间件（MQ）实现任务的持久化与补偿。
    
- **标准回答**：
    
    - 默认策略如 `AbortPolicy` 会抛异常，`CallerRunsPolicy` 会由调用者执行（降低入口压力）。
        
    - **自研建议**：自定义策略，将任务持久化到 Redis/DB，或重新放入 MQ 队列。
        
- **详细解析**： 线程池执行逻辑：核心线程 -> 阻塞队列 -> 最大线程 -> 拒绝策略。
    
- **架构深度**： 在高可用的微服务架构中，可以动态调整线程池参数（如美团开源的 DynamicTp）。当队列积压超过 80% 时，通过配置中心（Nacos）动态增大 `maximumPoolSize`，实现自适应流量调节。
    

#### Q019：JVM 内存模型（JMM）中的 `volatile` 关键字如何保证可见性？它能解决原子性问题吗？

- **考察点**：内存屏障（Memory Barrier）、缓存一致性协议（MESI）、指令重排序。
    
- **深度角度**：底层汇编指令 `lock` 前缀的作用。
    
- **标准回答**：
    
    - **可见性**：通过强制将修改后的值写回主内存，并失效其他处理器的缓存。
        
    - **原子性**：`volatile` 无法保证原子性（如 `i++`），原子性需依靠 `CAS` 或锁。
        
- **详细解析**： `volatile` 禁止指令重排，确保了双重检查锁定（DCL）单例模式的安全性。它在写操作后加入 Store 屏障，读操作前加入 Load 屏障。
    
- **架构深度**： 在 K8s 这种多实例部署环境下，单机的 `volatile` 只能解决单进程内的可见性。跨进程的状态同步必须依赖 **Redis** 的分布式信号量或 **ZooKeeper** 的监视机制。

#### Q020：Spring Bean 的循环依赖是如何通过三级缓存解决的？为什么二级缓存不行？

- **考察点**：Bean 声明周期、提前暴露（Early Reference）、动态代理（AOP）。
    
- **深度角度**：三级缓存中 `ObjectFactory` 对 AOP 代理对象生成的时机处理。
    
- **标准回答**：
    
    - **一级缓存**：存放完全初始化好的 Bean。
        
    - **二级缓存**：存放半成品 Bean（未注入属性）。
        
    - **三级缓存**：存放 `ObjectFactory`（工厂 lambda）。
        
    - **原因**：三级缓存主要是为了处理 **AOP**。如果只有二级缓存，Bean 在创建时就必须完成代理，这违反了 Spring 在 Bean 初始化后期才创建代理的设计原则。
        
- **详细解析**： Spring 默认支持单例 Bean 的 setter 循环依赖。当 A 依赖 B，B 又依赖 A 时，A 先将自己的工厂放入三级缓存，B 创建时通过工厂获取 A 的引用（如果是代理对象，则此时提前创建代理），从而完成闭环。
    
- **架构深度**： 构造器循环依赖无法通过缓存解决。在微服务架构中，应尽量通过 **业务解耦** 或 **@Lazy** 注解来规避循环依赖，因为过多的循环依赖往往意味着类职责划分不清晰。
    

#### Q021：SpringBoot 的自动装配（Auto-Configuration）底层是如何通过 SPI 机制实现的？

- **考察点**：`@EnableAutoConfiguration`、`spring.factories`、条件装配（Conditional）。
    
- **深度角度**：分析 `ConfigurationClassPostProcessor` 的加载顺序。
    
- **标准回答**：
    
    - 核心是 `@SpringBootApplication` 下的 `@EnableAutoConfiguration` 注解。
        
    - 利用 `SpringFactoriesLoader` 扫描所有 Jar 包下的 `META-INF/spring.factories` 文件。
        
    - 结合 `@ConditionalOnClass` 等条件注解按需加载。
        
- **详细解析**： Spring Boot 在启动时会扫描指定的配置文件，将符合条件的配置类注入到 IOC 容器。这实现了“开箱即用”的特性。
    
- **架构深度**： 在公司内部开发通用 Starter 时，可以通过自定义条件注解实现“功能开关”。例如：根据 K8s 环境变量是否存在来决定是否启动内置的监控 Sidecar 接口。
    

#### Q022：Spring Cloud Gateway 如何利用 Sentinel 实现“全链路灰度”下的细粒度限流？

- **考察点**：路由谓词、热点参数限流、Context 透传。
    
- **深度角度**：如何在网关层根据 Header 中的灰度标签进行不同的限流阈值计算。
    
- **标准回答**：
    
    - 在网关层集成 `sentinel-spring-cloud-gateway-adapter`。
        
    - 定义 `SentinelGatewayFilter` 并通过 `RequestItemParser` 提取 Header 里的灰度标识作为热点参数。
        
- **详细解析**： 通过对不同版本的流量设置不同的限流策略，可以保证灰度版本即使出现异常，也不会影响主版本。
    
- **架构深度**： 全链路灰度的难点在于 **TraceID 和 Tag 的跨线程透传**。在异步调用（WebFlux）环境下，需要自定义 `Context` 装饰器，确保标签在 Netty 线程间正确流动。
    

#### Q023：MyBatis 的插件（Plugin）机制是如何通过责任链模式影响 SQL 执行的？

- **考察点**：拦截器接口、动态代理、四大核心对象（Executor, StatementHandler等）。
    
- **深度角度**：如何通过插件实现自动化的“数据脱敏”或“多租户隔离”。
    
- **标准回答**：
    
    - MyBatis 允许拦截四大核心对象的调用。
        
    - 采用 **JDK 动态代理** 为目标对象生成代理类，并通过 `InterceptorChain` 形成责任链。
        
- **详细解析**： 开发者可以通过实现 `Interceptor` 接口，在 SQL 执行前修改 BoundSql（例如自动拼接 `where tenant_id = xxx`），实现多租户逻辑。
    
- **架构深度**： 在分库分表实战中，如果不使用 ShardingSphere，可以自研 MyBatis 插件拦截 `StatementHandler`，根据逻辑表名和分片算法动态改写真实的物理表名。

#### Q024：K8s 中的 Pod 优雅停机（Graceful Shutdown）时，JVM 进程无法正常退出怎么办？

- **考察点**：信号量传递（SIGTERM）、PID 1 问题、TerminationGracePeriodSeconds。
    
- **深度角度**：Docker 镜像启动脚本（shell vs exec 形式）对信号屏蔽的影响。
    
- **标准回答**：
    
    - 必须使用 `exec` 模式启动 JVM（例如 `ENTRYPOINT ["java", "-jar", ...]`），确保 java 进程是 PID 1。
        
    - 如果使用 shell 脚本启动，脚本会屏蔽 `SIGTERM` 信号，导致 JVM 无法执行 `ShutdownHook`。
        
- **详细解析**： K8s 会发送 `SIGTERM` 给容器，若容器在规定时间（默认 30s）内不退出，则强制 `SIGKILL`。如果 JVM 没收到信号，会导致正在处理的订单丢失或 Consul 注册中心状态未清除。
    
- **架构深度**： 结合 Spring Boot 的 `server.shutdown=graceful` 配置，可以确保 Tomcat 在关闭前不再接收新请求，并处理完当前线程池内的任务。
    

#### Q025：如何在 K8s 中解决 Spring Cloud 微服务的“服务发现”延迟问题？

- **考察点**：Nacos/Eureka 延迟、ReadinessProbe、Endpoints 同步。
    
- **深度角度**：应用启动完成到真正能接收流量之间的时间窗口治理。
    
- **标准回答**：
    
    - 现象：Pod 启动了但 Nacos 还没感知，或者 Pod 下线了 Nacos 还有缓存。
        
    - 解决：1. 缩短注册中心心跳频率；2. 合理配置 K8s 的 `initialDelaySeconds`。
        
- **详细解析**：
    
    K8s 的 Service 是由 `kube-proxy` 准实时更新的。如果使用 Spring Cloud 原生的服务发现，建议通过 `PreStop` 脚本在下线前显式调用 Nacos API 注销服务。
    
- **架构深度**： 在云原生环境下，最终趋势是 **云原生服务发现**（使用 K8s Service Name），通过 CoreDNS 解决，从而摆脱对第三方注册中心的依赖。
    

#### Q026：K8s 环境下如何通过 Prometheus 与 Grafana 实现对 JVM 垃圾回收的实时监控？

- **考察点**：JMX Exporter、自定义 Metrics、Grafana 面板。
    
- **深度角度**：如何设置合理的 GC 频率与时长报警阈值。
    
- **标准回答**：
    
    - 在 Pod 内集成 `jmx_exporter` 代理。
        
    - Prometheus 开启抓取（Scrape）任务。
        
    - 监控核心指标：`jvm_gc_collection_seconds_sum`（GC总耗时）和堆内存利用率。
        
- **详细解析**： 如果发现 GC 耗时占比超过 5%，通常意味着堆压力过大或参数设置不合理。
    
- **架构深度**： 应将 JVM 监控与容器监控（Node Exporter）结合。有时候 Full GC 是因为容器被 OOM 限流导致 CPU 资源不足，GC 线程跑不动。
    

#### Q027：当 K8s 节点（Node）发生压力时，Pod 的调度策略是如何保证核心服务不被“驱逐”的？

- **考察点**：QoS Classes (Guaranteed, Burstable, BestEffort)、Pod 优先级（PriorityClass）。
    
- **深度角度**：Request 与 Limit 设置对 Pod 稳定性的影响。
    
- **标准回答**：
    
    - 将 Request 和 Limit 设置为相等，使 Pod 获得 **Guaranteed** 级别。
        
    - 设置 `PriorityClass` 确保核心业务 Pod 在资源紧张时优先保留。
        
- **详细解析**： K8s 驱逐策略是先杀 `BestEffort`（没设置资源的），再杀 `Burstable`。
    
- **架构深度**： 架构设计中，数据库或核心网关必须是 `Guaranteed` 等级，而定时任务或实验性服务可以设为 `Burstable` 以提高集群资源利用率。
    

#### Q028：Spring Cloud 应用在容器内频繁报连接池满，但 CPU 不高，如何排查？

- **考察点**：连接泄露、数据库连接池配置、TCP 半链接。
    
- **深度角度**：容器内 `file-max` 限制与数据库 Server 端 `max_connections` 的匹配。
    
- **标准回答**：
    
    - 使用 `jstack` 查看线程状态，看是否大量阻塞在获取连接的地方。
        
    - 检查连接池（如 Druid/HikariCP）的活跃连接数与空闲连接数。
        
- **详细解析**： 通常是由于业务代码查询过慢，导致连接长时间不释放。也可能是容器的网络丢包导致连接断开，但连接池认为连接有效。
    
- **架构深度**： 在 K8s 中，应开启 **Istio 的连接池管理**（DestinationRule），在网关层做溢出控制，保护后端微服务不被压垮。
    

#### Q029：如何实现基于 K8s HPA 的 Spring Cloud 微服务“自动弹性伸缩”？

- **考察点**：Metrics Server、HPA 算法、冷却时间。
    
- **深度角度**：基于自定义业务指标（如 MQ 积压量）而非 CPU 进行伸缩。
    
- **标准回答**：
    
    - 默认 HPA 基于 CPU/内存，通过 `HorizontalPodAutoscaler` 定义。
        
    - 业务指标伸缩：使用 **KEDA**（Kubernetes-based Event Driven Autoscaling）。
        
- **详细解析**： 例如消费者 Pod，当 MQ 堆积超过 10 万条时，KEDA 自动触发 HPA 将副本从 2 扩展到 10。
    
- **架构深度**： 弹性伸缩必须考虑数据库的承载能力。如果微服务无节制扩容，最终会把单机数据库连接池瞬间占满，导致系统崩溃。
    

#### Q030：在大规模微服务系统中，如何治理“循环依赖的服务调用”？

- **考察点**：服务解耦、领域驱动设计（DDD）、异步化。
    
- **深度角度**：如何通过数据同步或事件驱动（Event-Driven）打破链路循环。
    
- **标准回答**：
    
    - 现象：A 调 B，B 调 C，C 调 A。这会导致分布式死锁或极难排查的递归故障。
        
    - 解决：1. 提取公共模块（服务合并）；2. **引入消息队列**，将同步调用改为异步通知。
        
- **详细解析**： C 不再直接调用 A，而是发送一个 `Message`，A 订阅该消息处理后续逻辑。
    
- **架构深度**： 这是 **DDD（领域驱动设计）** 的核心应用。如果两个服务必须循环调用，说明它们的限界上下文（Bounded Context）划分错误，应考虑合并为一个微服务。