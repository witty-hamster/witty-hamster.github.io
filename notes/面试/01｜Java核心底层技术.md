## 一、基础

### Q001：什么是 Java 的四种引用类型（强、软、弱、虚）？它们分别在什么场景下被回收？

- **回答方式一（定义流）**：强引用即使 OOM 也不会回收；软引用（Soft）在内存不足时回收，常用于缓存；弱引用（Weak）只要 GC 发现就回收，常用于 `ThreadLocal`；虚引用（Phantom）主要用于跟踪对象被回收的活动。
    
- **回答方式二（生命流）**：强引用是“救命稻草”，死也不松手；软引用是“食之无味”，内存紧了就扔；弱引用是“过眼云烟”，风吹（GC）即散；虚引用是“特工标记”，它不影响生存，只负责在对象死后发个信号。

---

## 二、集合
### Q001：HashMap 在 JDK 8 中仍存在哪些安全隐患？`ConcurrentHashMap` 的 `size()` 是如何计算的？

- **回答方式一（源码细节流）**：JDK 8 虽解决了死循环，但仍存在**数据覆盖**问题（并发 put 导致的索引覆盖）。`ConcurrentHashMap` 通过类似 `LongAdder` 的 `CounterCell` 数组分段计数，最后将 `baseCount` 与所有 `Cell` 值求和，避免了全局锁。
    
- **回答方式二（生活逻辑流）**：两个人同时往一个格子里放东西，可能后放的会把先放的给盖住。数人数时，不需要全校排一队，而是各班数各班的，最后校长把数汇总。

---

## 三、多线程
### Q001：在多线程中，什么是“虚假唤醒（Spurious Wakeup）”？为什么`wait()` 方法必须放在`while`循环中而不是`if` 中？

- **回答方式一（并发协议流）**：虚假唤醒是指线程在没有收到显式信号的情况下从 `wait` 状态醒来。放在 `while` 循环中是为了在醒来后重新检查业务条件。如果用 `if`，一旦发生虚假唤醒，线程会跳过条件检查直接执行后续逻辑，导致业务状态错误（如从空队列取数据）。
    
- **回答方式二（逻辑流）**：就像你在等外卖（条件）。外卖没到你睡着了（wait）。如果邻居突然关门响了一声（虚假唤醒）把你吵醒，你不能直接开门拿外卖，必须抬头看一眼外卖到底到没到（重新检查条件），所以得用 `while` 循环盯着。

### Q002：传统的 `Future` 在处理异步结果时有什么局限性？Java 8 引入的 `CompletableFuture` 是如何解决这些问题的？请描述其内部的“回调链”执行逻辑。

- **回答方式一（架构演进流）**：传统 `Future` 最大的问题是**阻塞**（必须调用 `.get()`）或**轮询**（`.isDone()`），且无法简单地将多个异步任务进行链式编排。`CompletableFuture` 借鉴了 Promise 思想，支持非阻塞的回调（如 `thenApply`, `thenCombine`）。其底层通过 `Completion` 对象构建一个单向链表栈，当任务完成后，会通过 `postComplete` 方法依次触发后续的回调节点。
    
- **回答方式二（场景模拟流）**：`Future` 就像你点完餐只能坐在窗口等（阻塞），或者不停去问“好了吗”（轮询）；`CompletableFuture` 则是留了电话，饭好了自动通知你，甚至能交待：“饭好了顺便帮我配个饮料（thenApply），然后把这两样一起打包送过来（thenCombine）”。这一切都是异步完成的，不需要你在那死等。

### Q003：`ThreadLocalMap` 的 Entry 为什么要将 Key 设置为弱引用？既然 Key 是弱引用，为什么还会发生内存泄漏？

- **回答方式一（引用链条流）**：Key 设置为弱引用是为了让 `ThreadLocal` 实例在没有外部强引用时能被 GC 回收。但 Entry 的 Value 是强引用，且被 `Thread` 对象持有。如果线程不结束（如在线程池中），Value 就永远无法回收。解决办法是每次使用完必须手动调用 `.remove()`。
    
- **回答方式二（清理责任流）**：Key 是弱引用，就像一张易碎的便签纸，风一吹（GC）就没了，避免了 Key 本身的堆积。但便签上指向的“包裹”（Value）是实打实的占地方。如果主人（线程）一直不死，这个包裹就会一直占着货架。所以离场时，你必须亲手把包裹（Value）也扔进垃圾桶。

### Q004：JUC 包下的锁和同步器（ReentrantLock, Semaphore, CountDownLatch）几乎都基于 AQS。请口述 AQS 内部是如何利用 `state` 变量和 CLH 变体队列实现线程排队的？

- **回答方式一（源码机制流）**：AQS 核心由 `volatile int state` 和一个双向 FIFO 队列组成。线程尝试通过 CAS 修改 `state`。若失败，则封装成 Node 节点加入队尾并挂起（LockSupport.park）。释放锁时，唤醒 Head 节点的后继节点。它巧妙利用了虚拟的“头节点”和节点状态（WaitStatus）来控制线程的唤醒与转让。
    
- **回答方式二（排队逻辑流）**：AQS 像是一个带自动门锁的办事大厅。`state` 就像门上的红绿灯（0开1关）。想进去的人先冲过去改灯，改失败的人就去排队（入队）。排在第一位的人会盯着前面那个人的后背（状态），当前面的人办完事走掉并拍拍他时，他才去抢灯进门。

### Q005：在多线程环境中，如何理解“伪共享（False Sharing）”对性能的影响？Java 8 是如何通过 `@Contended` 解决的？

- **回答方式一（硬件缓存流）**：CPU 缓存以缓存行（Cache Line，通常 64 字节）为单位。如果多个独立变量位于同一缓存行，不同 CPU 核心修改它们会导致缓存一致性协议（MESI）频繁触发缓存失效。`@Contended` 会在变量前后填充空字节，确保变量独占一个缓存行。
    
- **回答方式二（排队类比流）**：就像并排的两个工位。A 同学在写字，B 同学在擦桌子，由于桌子（缓存行）是一体的，B 擦桌子时 A 的纸也会抖，导致 A 没法写（缓存失效）。解决办法是给两个工位中间加个过道（填充字节），互不干扰。

### Q006：ForkJoinPool 的“工作窃取（Work-Stealing）”算法是如何平衡 CPU 利用率的？与普通的 ThreadPoolExecutor 有何区别？

- **回答方式一（调度算法流）**：普通线程池共享一个阻塞队列，存在严重的竞争。ForkJoinPool 每个线程有自己的双端队列（Deque）。当某个线程闲置时，会从其他繁忙线程队列的**尾部**“窃取”任务执行，减少了获取任务的冲突并充分利用了多核算力。
    
- **回答方式二（分包策略流）**：就像搬砖，普通线程池是所有人排一队领砖；ForkJoinPool 是每个人分一堆。我搬完了看你还没搬完，我就从你那一堆的底下（减少干扰）抽几块过来帮搬，保证所有人都不闲着。

---

## 四、锁
### Q001：CountDownLatch vs CyclicBarrier ，这两者在实现原理和应用场景上有何本质区别？为什么 `CyclicBarrier` 被称为“可循环使用”的？

- **回答方式一（底层对比流）**：`CountDownLatch` 基于 AQS 的共享锁实现，计数器减到 0 后无法重置，侧重于一个或多个线程等待其他线程完成。`CyclicBarrier` 基于 `ReentrantLock` 和 `Condition` 实现，所有线程在 Barrier 处互相等待，计数器到 0 后会自动重置并支持执行一个 `barrierAction`，侧重于多个线程之间的相互等待与协同。
    
- **回答方式二（任务类比流）**：`CountDownLatch` 像是一场考试，监考老师（主线程）得等所有学生（子线程）交卷了才能收工，考完就散场；`CyclicBarrier` 像是一次团队拓展，所有人必须在每个关卡集合（同步点）后才能一起进下一关，进完下一关，这个集合点还可以继续给下一波人用。

### Q002：`ReentrantReadWriteLock` 在读多写少的场景下已经很快了，为什么 Java 8 还要引入 `StampedLock`？它的“乐观读”是如何避免写线程饥饿的？

- **回答方式一（锁竞争优化流）**：读写锁在读的时候会阻塞写，如果读请求源源不断，写线程会“饥饿”。`StampedLock` 提供了 `tryOptimisticRead`，它在读取时完全不加锁，而是返回一个版本戳（Stamp）。读取后再验证 Stamp 是否被修改，若未修改则说明数据一致。这种方式让读写可以完全并行。
    
- **回答方式二（乐观校验流）**：以前是“进屋看书前必须把门反锁，不让写字的人进来”；`StampedLock` 的乐观读是“我直接进去看，但我记下当时家具的位置（版本号），看完后我再扫一眼家具动过没，如果没动过，说明我看的内容是准的”。如果动过，我再老老实实加锁重读一遍。

---

## 五、JVM
### Q001：JVM 对象内存布局中，Mark Word 的动态变化过程是怎样的？它是如何支撑 synchronized 锁升级的？

- **回答方式一（底层原理）**：对象头中的 Mark Word 存储了哈希码、GC 年龄和锁状态。锁升级过程：无锁状态下存储哈希码；偏向锁时存储线程 ID；轻量级锁时指向线程栈中的 Lock Record；重量级锁时指向操作系统的互斥量（Monitor）。这种设计避免了直接使用重量级锁带来的用户态与内核态切换开销。
    
- **回答方式二（状态机流）**：就像一把“智能锁”。平时没人用是开着的（无锁）；只有我一个人用，我就在上面贴个名字（偏向锁）；有两三个人抢，大家就比谁动作快（CAS 自旋，轻量级锁）；如果人太多，大家就去排队领号（重量级锁），由管家（内核）统一分配。


### Q002：在 Java 17 环境下，G1 收集器如何通过“预测模型”控制停顿时间？它与 CMS 的碎片整理机制有何本质不同？

- **回答方式一（深度原理解析）**：G1 将堆内存划分为多个大小相等的 Region。它通过跟踪每个 Region 的回收价值（可回收空间与预估耗时），在用户设定的 `MaxGCPauseMillis` 时间内，优先回收收益最大的 Region。不同于 CMS 采用的“标记-清除”算法，G1 采用“标记-复制”算法，在回收时自然完成内存压缩，从根本上规避了内存碎片化导致的 Full GC。
    
- **回答方式二（形象类比）**：CMS 像是在原地扫地，虽然快但家具（内存）乱了要挪位时就容易卡住；G1 则是把有用的东西搬到空房间，旧房间直接清空。G1 还会像导游一样掐表，如果 100ms 只够搬 5 个房间，它绝不搬第 6 个，从而保证准时“集合”。

### Q003：请解释 JMM 中的 `happens-before` 原则，并说明 `volatile` 是如何禁止指令重排序的？

- **回答方式一（底层协议流）**：`happens-before` 定义了操作间的内存可见性约束。对于 `volatile`，JVM 会在编译器和处理器层面插入**内存屏障**（Memory Barrier）。在写操作后插入 Store 屏障，读操作前插入 Load 屏障，确保缓存失效并强行刷回主存，阻止屏障前后的指令跨行重排。
    
- **回答方式二（通俗规则流）**：这就像一套“交通规则”，保证前车的动作必须对后车可见。`volatile` 就像在代码里筑了一堵墙（屏障），规定路上的指令不能越过这堵墙乱跑，必须严格按先后顺序执行。

---

## 六、IO

### Q001：Java NIO 堆外内存（Direct Memory）溢出该如何排查？为什么它不被 JVM 垃圾回收器直接管理？

- **回答方式一（内存治理流）**：堆外内存通过 `Unsafe` 或 `ByteBuffer.allocateDirect` 分配。GC 只能回收 `DirectByteBuffer` 对象，无法直接回收其背后的物理内存。通常需通过 `jstat` 观察 `Non-Heap` 占用，或使用 `NMT (Native Memory Tracking)` 追踪内存分配情况。
    
- **回答方式二（数据交换流）**：就像在机场外面租了个仓库。JVM 里面的环卫工（GC）扫不到外面。由于它减少了内核态与用户态的数据拷贝（零拷贝），所以传输快，但如果你只管租（分配）不管退（释放），机场外面（系统内存）就会爆掉。

### Q002：请从操作系统的角度，描述同步阻塞（BIO）、同步非阻塞（NIO）与异步非阻塞（AIO）在处理一个 TCP 连接请求时的具体区别？为什么在 Linux 环境下 AIO 的应用并不广泛？

- **回答方式一（系统调用流）**：BIO 在 `read` 时会一直阻塞直到内核缓冲区数据准备好并拷贝到用户空间。NIO 利用 `Selector` 和 `epoll`，通过轮询就绪状态来实现非阻塞，数据拷贝过程仍由用户线程完成。AIO 则是真正的异步，由内核完成数据拷贝后通知用户线程。在 Linux 中，AIO 的底层实现（如 `io_uring` 之前）并不成熟且性能提升有限，因此 NIO 配合 Netty 依然是当前主流。
    
- **回答方式二（场景比喻流）**：BIO 是你去钓鱼，盯着浮标一动不动，鱼不上钩你啥也干不了；NIO 是你插了很多鱼竿，你在岸边巡逻（轮询），看哪个浮标动了就去拉哪根杆（自己动手拉）；AIO 是你雇了个专业渔夫，鱼钓上来并处理好之后直接送到你家门口，你只需要负责吃（处理结果）。

### Q003：在高并发连接场景下，为什么 `epoll` 的性能远高于 `select`？请口述 `epoll` 的红黑树和就绪链表在其中起到的作用。

- **回答方式一（数据结构流）**：`select` 每次调用都要将全部文件描述符（FD）集合从用户态拷贝到内核态，并进行 O(n) 遍历。`epoll` 通过 `epoll_ctl` 在内核维护一棵红黑树来管理所有监控的 FD，仅需拷贝一次。当 FD 就绪时，通过回调机制将其放入一个双向就绪链表，用户只需 O(1) 地获取链表中的就绪 FD，彻底解决了海量连接下的遍历性能瓶颈。
    
- **回答方式二（管理效率流）**：`select` 就像班长点名，每次都要把全班名单（FD 集合）重写一遍发给老师，老师再一个个问谁有事（轮询）。`epoll` 则是班长在内核办了个“登记处”（红黑树），谁有事自己去登记处挂个铃铛（回调并进入链表），老师只需看一眼铃铛墙就知道谁有事，效率高下立判。

### Q004：Kafka 能够实现超高吞吐量离不开零拷贝技术。请解释 Java NIO 中 `FileChannel.transferTo()` 底层是如何减少上下文切换和数据拷贝次数的？`mmap` 与 `sendfile` 有什么区别？

- **回答方式一（内存管理流）**：传统 IO 需要 4 次拷贝和 4 次上下文切换。`sendfile` 机制下，数据直接从 Read Buffer 拷贝到 Socket Buffer（甚至通过 DMA 收集直接发往协议栈），不经过用户空间。`mmap` 则是将文件映射到内核空间缓冲区，用户态和内核态共享这一块内存。`sendfile` 适合直接发送，`mmap` 适合对文件内容还需要进行小规模逻辑处理的场景。
    
- **回答方式二（数据路径流）**：传统方式像搬家：把货从旧屋搬上货车，再从货车搬下到新屋。零拷贝则是直接把旧屋的地契改个名（映射），或者直接用车把货从旧仓库拉到新码头（内核态直接转发），中间不进你的个人办公室（用户空间），省去了大量搬运工（CPU）的体力活。

### Q005：Netty 为什么推荐使用主从多线程 Reactor 模型？`BossGroup` 和 `WorkerGroup` 分别承担什么职责？

- **回答方式一（架构设计流）**：单线程 Reactor 在高并发下无法发挥多核优势且易阻塞。主从模型中，`BossGroup` 仅负责 `Accept` 连接请求并将其注册到 `WorkerGroup` 的 `Selector` 上。`WorkerGroup` 负责处理具体的 `Read`、`Decode`、`Compute`、`Encode` 和 `Write`。这种职责分离确保了连接接入的及时性，也保证了业务处理的并发度。
    
- **回答方式二（分工协作流）**：这就像一家火锅店。`BossGroup` 是门口的迎宾小姐，只负责领位、登记名字；`WorkerGroup` 是服务员，负责点菜、上菜、加水。如果让迎宾小姐又领位又端菜，门口就会排长队；如果分工明确，客人进来得快，服务也转得开。

### Q006：Java 中 `DirectByteBuffer` 分配的是堆外内存，它如何避免 Full GC 扫描？在进行跨内存的数据读写时，是如何保证内存可见性的？

- **回答方式一（垃圾回收流）**：堆外内存由操作系统直接分配，不属于 JVM 堆。`DirectByteBuffer` 内部持有一个 `Cleaner` 对象（虚引用），当该 Buffer 对象被 GC 回收时，会触发回调执行堆外内存的释放。在读写底层，通过 `Unsafe` 类调用内存屏障指令（Fence），强制刷新 CPU 缓存，保证了多线程环境下对该内存块操作的有序性和可见性。
    
- **回答方式二（安全管理流）**：堆外内存就像你在公司外面租了个临时仓库（不受公司行政部管）。虽然行政部（GC）不扫这个仓库，但如果你把仓库钥匙（Buffer 对象）弄丢了，行政部在清理你的个人物品时会发现钥匙，并顺便通知房东退租。读写时的内存屏障就像是仓库的实时监控，保证每个人看仓库里的货都是最新状态。



